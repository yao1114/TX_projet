{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段代码是一个机器学习流程，用于训练和评估决策树分类器的性能。以下是每个部分的解释：\n",
    "\n",
    "1. 导入必要的库：pandas、numpy、scikit-learn的DecisionTreeClassifier、GridSearchCV、train_test_split、Pipeline和评价指标（accuracy_score，confusion_matrix，roc_auc_score和f1_score）。\n",
    "\n",
    "2. 定义函数read_and_split_data(file_path, test_size=0.2, random_state=40)，用于读取数据文件，将数据集分为训练集和测试集，并对目标变量进行预处理（将原来的1和3替换为2，2替换为1）。\n",
    "\n",
    "3. 定义函数evaluate_model_performance(model, X_test, y_test)，用于评估模型在测试集上的性能，包括准确率（accuracy）、混淆矩阵（confusion matrix）、AUC和F1值。\n",
    "\n",
    "4. 定义Pipeline对象pipeline，其中包含了决策树分类器（DecisionTreeClassifier）。\n",
    "\n",
    "5. 定义超参数空间param_grid，包含了分类器的最大深度、最小样本分割和最小样本叶子等参数。\n",
    "\n",
    "6. 使用GridSearchCV对象grid_search来寻找最佳超参数组合，通过5折交叉验证和网格搜索的方式来搜索最佳超参数，评价指标是准确率（accuracy）。\n",
    "\n",
    "7. 调用read_and_split_data函数读取和分割数据集，将训练集和测试集存储到X_train、X_test、y_train、y_test变量中。\n",
    "\n",
    "8. 在训练集上拟合GridSearchCV对象grid_search，寻找最佳超参数组合。\n",
    "\n",
    "9. 打印最佳超参数组合。\n",
    "\n",
    "10. 使用最佳超参数组合训练DecisionTreeClassifier对象best_model。\n",
    "\n",
    "11. 在测试集上评估最佳模型best_model的性能。\n",
    "\n",
    "这段代码的目的是通过优化决策树分类器的超参数来提高其在测试集上的性能。GridSearchCV用于在超参数空间中进行网格搜索，寻找最佳超参数组合。Pipeline用于将数据预处理和模型训练和评估链接在一起。最终，我们可以使用最佳模型来预测新数据的目标变量。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "在机器学习的实践中，数据预处理和模型训练是一个必要的过程，而管道（Pipeline）是一个将数据预处理和模型训练结合起来的工具。管道可以将多个预处理步骤和模型训练步骤组合成一个单一的对象，使得整个流程更加规范化、简洁和易于管理。以下是一些建立管道的好处：\n",
    "\n",
    "提高效率：使用管道可以减少重复的代码编写，从而提高编写代码和实验的效率。\n",
    "\n",
    "简化代码：将多个步骤组合成管道后，可以将大量的代码组织在一起，从而简化代码。\n",
    "\n",
    "避免数据泄露：当进行交叉验证时，需要对每个折叠的数据进行预处理。如果每次预处理都是手动完成的，那么就可能会发生数据泄露的情况。使用管道可以保证在交叉验证过程中，每个折叠都使用独立的预处理过程。\n",
    "\n",
    "可重复性：使用管道可以轻松地重复实验，并且可以使用相同的代码和管道对新的数据进行预测。\n",
    "\n",
    "易于管理：将多个步骤组合成管道后，可以轻松地管理整个过程，从而更容易地进行实验和进行模型的优化。\n",
    "\n",
    "综上所述，建立管道可以提高效率、简化代码、避免数据泄露、保证可重复性和易于管理。在机器学习实践中，建立管道是一个非常实用的工具。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'classifier__max_depth': 15, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2}\n",
      "Accuracy:  0.7953964194373402\n",
      "Confusion Matrix: \n",
      " [[0.8128655  0.1871345 ]\n",
      " [0.21818182 0.78181818]]\n",
      "AUC:  0.7973418394471026\n",
      "F1-score:  0.7765363128491621\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, f1_score\n",
    "\n",
    "# Define a function to read and split the dataset\n",
    "def read_and_split_data(file_path, test_size=0.2, random_state=40):\n",
    "    data = pd.read_csv(file_path)\n",
    "    # Copy target variable\n",
    "    y = data['session'].copy()\n",
    "    # Replace 1 and 3 with 2, and 2 with 1 in y\n",
    "    y.replace({1: 2, 2: 1, 3: 2}, inplace=True)\n",
    "    # Split the dataset into a training set and a test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, :-1], y, test_size=test_size, \n",
    "                                                        stratify=y, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Define a function to evaluate model performance\n",
    "def evaluate_model_performance(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "    print('Confusion Matrix: \\n', confusion_matrix(y_test, y_pred, normalize='true'))\n",
    "    print('AUC: ', roc_auc_score(y_test, y_pred))\n",
    "    print('F1-score: ', f1_score(y_test, y_pred))\n",
    "\n",
    "# Define a pipeline to link data preprocessing and model training and evaluation\n",
    "pipeline = Pipeline([\n",
    "    ('classifier', DecisionTreeClassifier(random_state=40))\n",
    "])\n",
    "\n",
    "# Define hyperparameter space\n",
    "param_grid = {\n",
    "    'classifier__max_depth': [2, 5, 10, 15],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=5, # 5-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Read and split the dataset\n",
    "X_train, X_test, y_train, y_test = read_and_split_data('HRV_ECG_step60.csv')\n",
    "\n",
    "# Fit GridSearchCV object on the training dataset\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print('Best parameters:', grid_search.best_params_)\n",
    "\n",
    "# Fit DecisionTreeClassifier object with best hyperparameters on the training dataset\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model performance\n",
    "evaluate_model_performance(best_model, X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'classifier__max_depth': 15, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2}\n",
      "Accuracy:  0.7953964194373402\n",
      "Confusion Matrix: \n",
      " [[0.8128655  0.1871345 ]\n",
      " [0.21818182 0.78181818]]\n",
      "AUC:  0.7973418394471026\n",
      "F1-score:  0.7765363128491621\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, f1_score\n",
    "\n",
    "# Define a function to read and split the dataset\n",
    "def read_and_split_data(file_path, test_size=0.2, random_state=40):\n",
    "    data = pd.read_csv(file_path)\n",
    "    # Copy target variable\n",
    "    y = data['session'].copy()\n",
    "    # Replace 1 and 3 with 2, and 2 with 1 in y\n",
    "    y.replace({1: 2, 2: 1, 3: 2}, inplace=True)\n",
    "    # Split the dataset into a training set and a test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, :-1], y, test_size=test_size, \n",
    "                                                        stratify=y, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Define a function to evaluate model performance\n",
    "def evaluate_model_performance(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "    print('Confusion Matrix: \\n', confusion_matrix(y_test, y_pred, normalize='true'))\n",
    "    print('AUC: ', roc_auc_score(y_test, y_pred))\n",
    "    print('F1-score: ', f1_score(y_test, y_pred))\n",
    "\n",
    "# Define a pipeline to link data preprocessing and model training and evaluation\n",
    "pipeline = Pipeline([\n",
    "    ('classifier', DecisionTreeClassifier(random_state=40))\n",
    "])\n",
    "\n",
    "# Define hyperparameter space\n",
    "param_grid = {\n",
    "    'classifier__max_depth': [2, 5, 10, 15],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=5, # 5-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Read and split the dataset\n",
    "X_train, X_test, y_train, y_test = read_and_split_data('HRV_ECG_step60.csv')\n",
    "\n",
    "# Fit GridSearchCV object on the training dataset\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print('Best parameters:', grid_search.best_params_)\n",
    "\n",
    "# Fit DecisionTreeClassifier object with best hyperparameters on the training dataset\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model performance\n",
    "evaluate_model_performance(best_model, X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'classifier__kernel': 'rbf', 'classifier__gamma': 0.1, 'classifier__C': 1000.0}\n",
      "Accuracy:  0.8567774936061381\n",
      "Confusion Matrix: \n",
      " [[139  32]\n",
      " [ 24 196]]\n",
      "AUC:  0.8518872939925571\n",
      "F1-score:  0.8323353293413173\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# 采用了随机搜索替代网格搜索，并且使用了核函数和特征缩放进行处理，以提高模型效率和性能\n",
    "\n",
    "# Define a function to read and split the dataset\n",
    "def read_and_split_data(file_path, test_size=0.2, random_state=40):\n",
    "    data = pd.read_csv(file_path)\n",
    "    # Copy target variable\n",
    "    y = data['session'].copy()\n",
    "    # Replace 1 and 3 with 2, and 2 with 1 in y\n",
    "    y.replace({1: 2, 2: 1, 3: 2}, inplace=True)\n",
    "    # Split the dataset into a training set and a test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, :-1], y, test_size=test_size, \n",
    "                                                        stratify=y, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Define a function to evaluate model performance\n",
    "def evaluate_model_performance(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "    print('Confusion Matrix: \\n', confusion_matrix(y_test, y_pred))\n",
    "    print('AUC: ', roc_auc_score(y_test, y_pred))\n",
    "    print('F1-score: ', f1_score(y_test, y_pred))\n",
    "\n",
    "# Define a pipeline to link data preprocessing and model training and evaluation\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', SVC(random_state=40))\n",
    "])\n",
    "\n",
    "# Define hyperparameter distributions for random search\n",
    "param_distributions = {\n",
    "    'classifier__C': np.logspace(-3, 3, 7),\n",
    "    'classifier__kernel': ['linear', 'rbf'],\n",
    "    'classifier__gamma': ['scale', 'auto'] + list(np.logspace(-3, 3, 7)),\n",
    "}\n",
    "\n",
    "# Use RandomizedSearchCV to find the best hyperparameters\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    cv=5, # 5-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    n_iter=50, # number of random search iterations\n",
    "    random_state=40,\n",
    ")\n",
    "\n",
    "# Read and split the dataset\n",
    "X_train, X_test, y_train, y_test = read_and_split_data('HRV_ECG_step60.csv')\n",
    "\n",
    "# Fit RandomizedSearchCV object on the training dataset\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print('Best parameters:', random_search.best_params_)\n",
    "\n",
    "# Fit SVM object with best hyperparameters on the training dataset\n",
    "best_model = random_search.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model performance\n",
    "evaluate_model_performance(best_model, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
