{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4feebde6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MutableMapping' from 'collections' (c:\\Users\\huxua\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\collections\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Tuple\n\u001b[0;32m     17\u001b[0m \u001b[39m# import hrvanalysis\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhrv\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclassical\u001b[39;00m \u001b[39mimport\u001b[39;00m frequency_domain, time_domain, non_linear\n\u001b[0;32m     19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpywt\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtsfel\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\huxua\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hrv\\__init__.py:20\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# coding: utf-8\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[39m# Copyright 2015-2016 Rhenan Bartels <https://github.com/rhenanbartels/hrv/>\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m \u001b[39m# General imports\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mhrv\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclassical\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mclassical\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mhrv\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mutils\u001b[39;00m\n\u001b[0;32m     23\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m0.2.10\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\huxua\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hrv\\classical.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msignal\u001b[39;00m \u001b[39mimport\u001b[39;00m welch\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspectrum\u001b[39;00m \u001b[39mimport\u001b[39;00m pburg\n\u001b[1;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhrv\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdetrend\u001b[39;00m \u001b[39mimport\u001b[39;00m polynomial_detrend\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhrv\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrri\u001b[39;00m \u001b[39mimport\u001b[39;00m RRi\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhrv\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (validate_rri, _interpolate_rri)\n",
      "File \u001b[1;32mc:\\Users\\huxua\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hrv\\detrend.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msignal\u001b[39;00m \u001b[39mimport\u001b[39;00m savgol_filter\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m \u001b[39mimport\u001b[39;00m spdiags, dia_matrix\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhrv\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrri\u001b[39;00m \u001b[39mimport\u001b[39;00m RRiDetrended, RRi, _create_time_array\n\u001b[0;32m      9\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mpolynomial_detrend\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msmoothness_priors\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msg_detrend\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpolynomial_detrend\u001b[39m(rri, degree\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\huxua\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hrv\\rri.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mObjects for dealing with RR intervals series (RRi).\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39m - `RRiDetrended` -- detrended RRi values\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39mimport\u001b[39;00m MutableMapping, defaultdict\n\u001b[0;32m     20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'MutableMapping' from 'collections' (c:\\Users\\huxua\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\collections\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta, time\n",
    "import scipy.io as sio\n",
    "from mat4py import loadmat\n",
    "from scipy.io import savemat\n",
    "from scipy.interpolate import interp1d\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy import signal\n",
    "import math\n",
    "import csv\n",
    "import time\n",
    "from os import path\n",
    "from typing import Tuple\n",
    "# import hrvanalysis\n",
    "from hrv.classical import frequency_domain, time_domain, non_linear\n",
    "import pywt\n",
    "import tsfel\n",
    "import biosppy\n",
    "import pyhrv.time_domain as td\n",
    "import pyhrv.frequency_domain as fd\n",
    "import pyhrv.tools as tools\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23b1e2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interp_condition(stats, window_len, max_missing):\n",
    "    #stats is a row of the stats file over each window with the following columns : \n",
    "                #'window','Start_time','End_time','file_duration','N_IBI','Number of ectopic RR',\n",
    "                #'% of ectopic RR','N_missing','%_missing','SNR_db','subject_id','Dass_before',\n",
    "                #'Dass_after','Q_before','Q_after'\n",
    "    # window_len : minimum window lenght (in seconds)\n",
    "    # max_missing : maximum % of missing data in a window (in %)\n",
    "    \n",
    "    bool_duration = stats['file_duration'] >= window_len\n",
    "    bool_missing_data = stats['%_missing'] <= max_missing\n",
    "    \n",
    "#     print(bool_duration[0])\n",
    "#     print(bool_missing_data[0])\n",
    "    \n",
    "    rejection = (bool_missing_data[0] == False) or (bool_duration[0]==False)\n",
    "#     print(rejection)\n",
    "    \n",
    "    return rejection\n",
    "\n",
    "def get_open_file(info,general_path,day,segment,subject_id):\n",
    "    \n",
    "    path = \"{0}/{1}/IBI_{1}_seg_{2}_subject_{3}.csv\".format(general_path,day,segment,subject_id)\n",
    "    df_file = pd.read_csv(path)\n",
    "    \n",
    "#     i = df_file[df_file['Datetime'] == info['Start_time']]\n",
    "#     j= df_file['Datetime'] == info['End_time']\n",
    "    \n",
    "#     print(i,j)\n",
    "    seg = df_file[(df_file['Datetime'] > info['Start_time'])  & (df_file['Datetime'] < info['End_time'])]\n",
    "    seg = seg.reset_index()\n",
    "    \n",
    "    return seg\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# @ @\n",
    "# Interpolates a set of RR interval signals to make them evenly spaced and returns the interpolated RR interval signals\n",
    "def standard_RR_interpolation(RRR,TTT):\n",
    "    \n",
    "#     subject_id = filename.rpartition['sujet_'][2]\n",
    "    R = RRR\n",
    "    T = TTT\n",
    "    \n",
    "    RR_interp = np.copy(R)\n",
    "    lim= len(RR_interp)\n",
    "       \n",
    "    T_interp = np.copy(T)\n",
    "    \n",
    "    RR_interp = pd.DataFrame(RR_interp)\n",
    "    #T_interp = pd.DataFrame(T_interp)\n",
    "    \n",
    "    N_vect = list()\n",
    "    interp_indices = list()\n",
    "    \n",
    "    for i in range(2,lim-1):\n",
    "        \n",
    "        T = pd.to_datetime(T)\n",
    "        T_start = T[i-1]\n",
    "        T_end = T[i]\n",
    "        \n",
    "        trou = (T_end-T_start)/np.timedelta64(1,'s')\n",
    "#         print(trou)\n",
    "        \n",
    "        #RR_mean = 1\n",
    "\n",
    "        if trou > 1.3:\n",
    "            \n",
    "        #************* Mean and std ************  \n",
    "            if i <11:\n",
    "                RR_mean = np.nanmean(R[0:i])\n",
    "            else:\n",
    "                RR_mean = np.nanmean(R[(i-10):i])\n",
    "            \n",
    "            #N = round(trou/RR_mean)\n",
    "            #print(i,trou, RR_mean)\n",
    "            N = math.floor(trou/RR_mean)\n",
    "            #print(type(N))\n",
    "            \n",
    "            N_vect = N_vect + [N]  # keep N number of Nan in the gap\n",
    "            interp_indices = interp_indices + [i]   #gap indice\n",
    "            \n",
    "    acc = 0\n",
    "    #print(len(RR_interp))\n",
    "    RR_interp = RR_interp.to_numpy()\n",
    "    \n",
    "    for i in range(len(N_vect)):\n",
    "        a = np.empty((N_vect[i],1))\n",
    "        a[:] = np.nan\n",
    "        \n",
    "        RR_interp = np.insert(RR_interp, interp_indices[i]+1,a,axis=0)\n",
    "\n",
    "        \n",
    "    \n",
    "    # Filtrage \n",
    "    for i in range(2,len(RR_interp)):\n",
    "        if RR_interp[i] < 0.3 or RR_interp[i]>1.3:\n",
    "            RR_interp[i] = np.nan\n",
    "        \n",
    "    \n",
    "    RR_pchip = pd.DataFrame(RR_interp).interpolate(method='pchip')\n",
    "           \n",
    "#     RR_pchip.to_csv(\"{0}/{1}_stan_spline.csv\".format(save_filepath, filename), index=False)\n",
    "    \n",
    "    return(RR_pchip)\n",
    "\n",
    "\n",
    "def RR_original_stats(R,T):\n",
    "        \n",
    "    RR_interp = np.copy(R)\n",
    "    lim= len(RR_interp)\n",
    "       \n",
    "    T_interp = np.copy(T)\n",
    "    \n",
    "    RR_interp = pd.DataFrame(RR_interp)\n",
    "    #T_interp = pd.DataFrame(T_interp)\n",
    "    \n",
    "    N_vect = list()\n",
    "    interp_indices = list()\n",
    "    \n",
    "    \n",
    "    # Get file duration\n",
    "    T = pd.to_datetime(T)\n",
    "    \n",
    "    T_0 = T.min()\n",
    "    T_last=T.max()\n",
    "\n",
    "#     print(T_0,T_last)\n",
    "#     file_duration=(T_last-T_0)/np.timedelta64(1,'s')\n",
    "    duration=(T_last-T_0)\n",
    "    file_duration=duration.total_seconds()\n",
    "    \n",
    "#     file_duration = time.strftime(\"%H:%M:%S\", time.gmtime(file_duration))\n",
    "#     print(file_duration)\n",
    "    \n",
    "    # Get missing data index\n",
    "    for i in range(2,lim-1):\n",
    "        \n",
    "        T = pd.to_datetime(T)\n",
    "        T_start = T[i-1]\n",
    "        T_end = T[i]\n",
    "        \n",
    "        trou = (T_end-T_start)/np.timedelta64(1,'s')\n",
    "        \n",
    "\n",
    "        if trou > 1.3:   \n",
    "        #************* Mean and std ************  \n",
    "            if i <11:\n",
    "                RR_mean = np.nanmean(R[0:i])\n",
    "            else:\n",
    "                RR_mean = np.nanmean(R[(i-10):i])\n",
    "            \n",
    "\n",
    "            N = math.floor(trou/RR_mean)\n",
    "#             print(trou,N)\n",
    "            \n",
    "            N_vect = N_vect + [N]  # keep N number of Nan in the gap\n",
    "            interp_indices = interp_indices + [i]   #gap indice\n",
    "            \n",
    "    acc = 0\n",
    "\n",
    "    \n",
    "    # STATS\n",
    "    ect_original =pd.DataFrame([len(R[R > 1.3])+ len(R[R <0.3])]) \n",
    "    \n",
    "    df=pd.DataFrame({'Starting_time':[T_0]})\n",
    "#     df['Starting_time']=T_0\n",
    "    df['End_time']=T_last\n",
    "    df['file_duration']=file_duration\n",
    "    df['N_IBI'] = lim\n",
    "    \n",
    "    \n",
    "    df['N_of_ectopic_RR']=ect_original\n",
    "    df['%_of_ectopic_RR'] =100*ect_original[0]/len(R)\n",
    "    \n",
    "    df['N_missing'] = sum(N_vect)\n",
    "    df['%_missing'] = 100*sum(N_vect)/(lim+sum(N_vect))\n",
    "\n",
    "    df['subject_id']= subject_id\n",
    "#     df['filename']= filename\n",
    "    \n",
    "    return(df)\n",
    "            \n",
    "           \n",
    "# stats_columns = ['file_duration','Number of ectopic RR','% of ectopic RR','N_missing','%_missing','SNR_db','subject_id','filename']\n",
    "\n",
    "\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "           \n",
    "\n",
    "        \n",
    "# @@\n",
    "# Returns a list containing the cumulative time (in milliseconds) between heartbeats\n",
    "def calculate_real_time(rrs):\n",
    "    acc = 0\n",
    "    real_time = []\n",
    "\n",
    "    for rr in rrs:\n",
    "        real_time.append(acc)\n",
    "        acc = acc + rr\n",
    "\n",
    "    return real_time\n",
    "\n",
    "\n",
    "def calc_true_hr(rrs):\n",
    "    hrs = []\n",
    "    for rr in rrs:\n",
    "        hr = 60 * 1000 / rr\n",
    "        hrs.append(hr)\n",
    "    return hrs\n",
    "\n",
    "def calc_true_time_domain(rrs, log):\n",
    "#     rrs = np.array(rr)\n",
    "#     print(rr)\n",
    "\n",
    "    output = pd.DataFrame()\n",
    "            \n",
    "        # Time Domain Features    \n",
    "#     tdom = time_domain(rrs)\n",
    "    tdom=td.time_domain(nni=rrs) # 时域特征\n",
    "    plt.close()\n",
    "        \n",
    "        # Non linear Domain\n",
    "    nl = non_linear(rrs) # 非线性特征\n",
    "        \n",
    "        # TSFEL statistical\n",
    "    vect = rrs\n",
    "    abs_energy =  tsfel.feature_extraction.features.abs_energy(vect)\n",
    "    autocorr =  tsfel.feature_extraction.features.autocorr(vect) \n",
    "    mean =  tsfel.feature_extraction.features.calc_mean(vect) \n",
    "    median =  tsfel.feature_extraction.features.calc_median(vect) \n",
    "    std =  tsfel.feature_extraction.features.calc_std(vect)\n",
    "    entropy =  tsfel.feature_extraction.features.entropy(vect) \n",
    "    interq_range =  tsfel.feature_extraction.features.interq_range(vect) \n",
    "    kurtosis =  tsfel.feature_extraction.features.kurtosis(vect) \n",
    "    mean_abs_dev =  tsfel.feature_extraction.features.mean_abs_deviation(vect) \n",
    "    mean_abs_diff =  tsfel.feature_extraction.features.mean_abs_diff(vect) \n",
    "    mean_diff =  tsfel.feature_extraction.features.mean_diff(vect) \n",
    "    median_abs_dev =  tsfel.feature_extraction.features.median_abs_deviation(vect) \n",
    "    median_abs_diff =  tsfel.feature_extraction.features.median_abs_diff(vect) \n",
    "    median_diff =  tsfel.feature_extraction.features.median_diff(vect) \n",
    "    skewness =  tsfel.feature_extraction.features.skewness(vect) \n",
    "    slope =  tsfel.feature_extraction.features.slope(vect) \n",
    "\n",
    "    tsfel_feat = {'abs_energy':abs_energy,'autocorr':autocorr,'mean':mean,'median':median,'std':std,'entropy':entropy,\n",
    "                  'interq_range':interq_range,'mean_abs_dev':mean_abs_dev,'mean_abs_diff':mean_abs_diff,\n",
    "                  'mean_diff':mean_diff,'median_abs_dev':median_abs_dev,'median_abs_diff':median_abs_diff,\n",
    "                  'median_diff':median_diff, 'kurtosis':kurtosis,'skewness':skewness,'slope':slope}\n",
    "\n",
    "        # TSFEL frequency\n",
    "        \n",
    "    up_sampled_signal = up_sample(rrs, 8)\n",
    "    vect = up_sampled_signal\n",
    "        \n",
    "    bw = tsfel.feature_extraction.features.power_bandwidth(vect, 8)\n",
    "    spec_ent = tsfel.feature_extraction.features.spectral_entropy(vect, 8)\n",
    "    spec_centroid = tsfel.feature_extraction.features.spectral_centroid(vect, 8)\n",
    "    spec_decr = tsfel.feature_extraction.features.spectral_decrease(vect, 8)\n",
    "    spec_dist = tsfel.feature_extraction.features.spectral_distance(vect, 8)\n",
    "    spec_kurt = tsfel.feature_extraction.features.spectral_kurtosis(vect, 8)\n",
    "    spec_roll_95 = tsfel.feature_extraction.features.spectral_roll_off(vect, 8)\n",
    "    spec_skew = tsfel.feature_extraction.features.spectral_skewness(vect, 8)\n",
    "    spec_var = tsfel.feature_extraction.features.spectral_variation(vect, 8)  \n",
    "    energy =  tsfel.feature_extraction.features.total_energy(vect, 8)\n",
    "    freq =  tsfel.feature_extraction.features.fundamental_frequency(vect, 8)\n",
    "    med_freq =  tsfel.feature_extraction.features.median_frequency(vect, 8)\n",
    "    auc =  tsfel.feature_extraction.features.auc(vect, 8) \n",
    "    h_e =  tsfel.feature_extraction.features.human_range_energy(vect, 8) \n",
    "        \n",
    "        \n",
    "\n",
    "    # 上采样后的信号中提取一系列频域特征\n",
    "    tsfel_feat_spec = {'bw':bw,'spec_ent':spec_ent,'spec_decr':spec_decr,' spec_dist': spec_dist,'spec_kurt':spec_kurt,\n",
    "                       ' spec_roll_95': spec_roll_95,'spec_skew':spec_skew,'spec_var':spec_var,'spec_centroid':spec_centroid,\n",
    "                       'energy':energy,'fundamental_freq':freq,'med_freq':med_freq,'auc':auc,'human_energy':h_e}\n",
    "    \n",
    "\n",
    "#     time = []\n",
    "\n",
    "#     for i in range(0, len(rrs)):\n",
    "#         time.append(float(i))\n",
    "#     print(time)\n",
    "\n",
    "    fdom = frequency_domain(rrs,interp_method=None)\n",
    "\n",
    "#         tps = {'Tps': j - 5}\n",
    "\n",
    "    output = output.append({**tdom,**tsfel_feat, **nl, **fdom,**tsfel_feat_spec},ignore_index=True)\n",
    "#         output = output.append({**tps, **tdom, **nl, **fdom}, ignore_index=True)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def up_sample(rr_intervals, frequency):\n",
    "    time = calculate_real_time(rr_intervals)\n",
    "\n",
    "    cs = interp1d(time, rr_intervals)\n",
    "    xs = np.arange(0, time[-1], 1000 / frequency)\n",
    "\n",
    "    cubic_spline = cs(xs)\n",
    "\n",
    "    return cubic_spline\n",
    "\n",
    "\n",
    "## This is the only funtion to call to compute HRV features\n",
    "def hrv_features(RR, log=False):\n",
    "    \n",
    "    # RR should be in ms\n",
    "    rr = RR.apply(lambda x: 1000 * x)\n",
    "    foo=rr.to_numpy().flatten()\n",
    "    \n",
    "    df_HRV_features = calc_true_time_domain(foo, log)    \n",
    "\n",
    "    return df_HRV_features\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ce27745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin, end 1970-01-01 00:27:42.465388795 1970-01-01 00:27:42.471885321\n",
      "window 300 seconds\n",
      "i=\n",
      "df_stats                   Starting_time                      End_time  file_duration  \\\n",
      "0 1970-01-01 00:27:42.465388795 1970-01-01 00:27:42.471885321       0.006496   \n",
      "\n",
      "   N_IBI  N_of_ectopic_RR  %_of_ectopic_RR  N_missing  %_missing subject_id  \n",
      "0   6314             6314            100.0          0        0.0        333  \n",
      "rejection True\n",
      "df_stats                   Starting_time                      End_time  file_duration  \\\n",
      "0 1970-01-01 00:27:42.465388795 1970-01-01 00:27:42.471885321       0.006496   \n",
      "\n",
      "   N_IBI  N_of_ectopic_RR  %_of_ectopic_RR  N_missing  %_missing subject_id  \n",
      "0   6314             6314            100.0          0        0.0        333  \n",
      "rejection True\n",
      "df_stats                   Starting_time                      End_time  file_duration  \\\n",
      "0 1970-01-01 00:27:42.465388795 1970-01-01 00:27:42.471885321       0.006496   \n",
      "\n",
      "   N_IBI  N_of_ectopic_RR  %_of_ectopic_RR  N_missing  %_missing subject_id  \n",
      "0   6314             6314            100.0          0        0.0        333  \n",
      "rejection True\n",
      "df_stats                   Starting_time                      End_time  file_duration  \\\n",
      "0 1970-01-01 00:27:42.465388795 1970-01-01 00:27:42.471885321       0.006496   \n",
      "\n",
      "   N_IBI  N_of_ectopic_RR  %_of_ectopic_RR  N_missing  %_missing subject_id  \n",
      "0   6314             6314            100.0          0        0.0        333  \n",
      "rejection True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"save_file = ('{0}/HRV_ECG_step{1}_mona.csv'.format(save_path,step))\\nwith open(save_file,'a') as f:\\n    writer = csv.writer(f)\\n    writer.writerow(df_features.columns)\\n    print('complete')\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "window_len= 300\n",
    "max_missing=50 # Maximum percentage of acceptable missing data in each window to compute HRV features\n",
    "wind = 300\n",
    "step=60 #overlap of 1min = 60s\n",
    "\n",
    "general_path = \"C:/Users/huxua/Desktop/TX/Dataset2\"\n",
    "# general_path = \"C:/Users/mouna/Desktop/experiment/Data/2_preprocessed_data/HRV/Signal_HRV_ECG\"\n",
    "\n",
    "save_path = \"C:/Users/huxua/Desktop/TX/Dataset1\"\n",
    "# try:\n",
    "#     os.mkdir(save_path)\n",
    "# except OSError:\n",
    "#     print('Creation failed or File already exists')\n",
    "\n",
    "# save_file = \"{0}/HRV_{1}_s_{2}_percent_step{3}.csv\".format(save_path,window_len,max_missing,step)\n",
    "\n",
    "global df_features\n",
    "\n",
    "for subfolder in os.listdir('C:/Users/huxua/Desktop/TX/Dataset2'): # path pour les fichiers HRV\n",
    "    for filename in os.listdir('C:/Users/huxua/Desktop/TX/Dataset2/'+subfolder):\n",
    "        open_path = ('C:/Users/huxua/Desktop/TX/Dataset2/'+subfolder +'/'+filename)\n",
    "\n",
    "#       #Remove extension xlsx from filename\n",
    "        file = filename.rpartition('.csv')[0]\n",
    "#             print(file)\n",
    "        subject_id = filename.rpartition('_drowsy')[0]\n",
    "                \n",
    "#         filepath = \"{0}/{1}\".format(open_path,filename)\n",
    "            \n",
    "        df_file = pd.read_csv('{0}'.format(open_path))\n",
    "\n",
    "        RR = df_file.RR_ST\n",
    "        T = df_file.RR_timestamps_ST\n",
    "        session = 0\n",
    "        T = pd.to_datetime(T)\n",
    "\n",
    "        begin, end = T.min(), T.max()\n",
    "        print('begin, end',begin, end)\n",
    "          \n",
    "        shift = np.timedelta64(step,'s')   # same as datetime unit\n",
    "        window = np.timedelta64(wind,'s')\n",
    "        print('window',window)\n",
    "        print('i=')\n",
    "        for i in range(int((end - begin - window)/shift) + 1):\n",
    "            print(i)\n",
    "\n",
    "        ## Split Signal into 5min windows\n",
    "        if len(RR)>10:\n",
    "\n",
    "            buffers_limit = [[begin + i * shift, begin + window + i * shift] for i in range(int((end - begin - window)/shift) + 1,1)]\n",
    "            buffers =  [df_file[(T >= buffer[0]) & (T < buffer[1])] for buffer in buffers_limit]\n",
    "\n",
    "            j=0\n",
    "            for buffer in buffers: \n",
    "                try:\n",
    "                    RR = buffer.RR_ST.to_numpy()\n",
    "                    T = buffer.RR_timestamps_ST.to_numpy()\n",
    "                \n",
    "                    df_stats = RR_original_stats(RR,T)\n",
    "                    rejection = interp_condition(df_stats, window_len, max_missing)\n",
    "                    \n",
    "                    print('df_stats',df_stats)\n",
    "                    print('rejection',rejection)\n",
    "          \n",
    "                    ## This is in order to reject segments with more than 50% of missing data\n",
    "                    if rejection==False:\n",
    "                \n",
    "                        interp_RR = standard_RR_interpolation(RR,T)  #interpolation\n",
    "                        df_features = hrv_features(interp_RR, log=False)     #compute HRV features\n",
    "            \n",
    "                        df_features['ID'] = subject_id\n",
    "                        df_features['session'] = session\n",
    "\n",
    "                        print('df inner',df_features)\n",
    "                    \n",
    "                    #Add features to csv database\n",
    "                        save_file = ('{0}/HRV_ECG_step{1}_mona.csv'.format(save_path,step))\n",
    "                        with open(save_file,'a') as f:\n",
    "                            writer = csv.writer(f)\n",
    "                            writer.writerow(df_features.iloc[0])\n",
    "                            print('complete')\n",
    "                except:\n",
    "                    print('error')\n",
    "\n",
    "'''save_file = ('{0}/HRV_ECG_step{1}_mona.csv'.format(save_path,step))\n",
    "with open(save_file,'a') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(df_features.columns)\n",
    "    print('complete')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7196b0bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ced6f69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
